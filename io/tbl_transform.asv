function tblOut = tbl_transform(tbl, varargin)
% TBL_TRANSFORM Applies log transformations, z-scoring, and normalization to table columns.
%
% SUMMARY:
% This function transforms table columns to improve their distributional
% properties for statistical analysis. It applies log transformation to
% highly skewed variables, z-scoring to standardise variables for
% regression analysis, and normalization relative to reference categories.
%
% INPUT (Required):
%   tbl         - Input table to be transformed.
%
% INPUT (Optional Key-Value Pairs):
%   varsExc     - Cell array of variable names to exclude from transformations {[]}.
%   varsInc     - Cell array of variable names to include in transformations {[]}.
%                  If provided, only these variables will be transformed (overrides varsExc).
%   flgZ        - Logical flag to apply z-scoring {true}.
%   flgLog      - Logical flag to apply log transformation to skewed variables {true}.
%   logBase     - Numeric or 'e'. Base for log transformation {10}.
%   flgLogit    - Logical flag to apply logit transformation {false}.
%   flgNorm     - Logical flag to apply normalization relative to reference category {false}.
%   skewThr     - Skewness threshold for log transformation {2}.
%   varsGrp     - Cell array of categorical variable names defining groups for
%                  separate transformation {[]}. If provided, transformations are
%                  applied separately within each group combination.
%   varNorm     - String. The name of the categorical variable whose reference
%                  category mean will be used for normalization (required if flgNorm=true).
%
% OUTPUT:
%   tblOut      - Transformed table with the same structure as input.
%
% EXAMPLE:
%   tblOut = tbl_transform(tbl, 'varsExc', {'UnitID', 'Group'}, 'flgZ', true);
%   tblOut = tbl_transform(tbl, 'varsGrp', {'Group', 'State'}, 'flgZ', true);
%   tblOut = tbl_transform(tbl, 'flgNorm', true, 'varNorm', 'Day', 'varsGrp', {'Group', 'State'});
%
% DEPENDENCIES:
%   None
%
%   See also: LME_ANALYSE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ARGUMENT PARSING & INITIALIZATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

p = inputParser;
addRequired(p, 'tbl', @istable);
addParameter(p, 'varsExc', [], @(x) iscell(x) || isempty(x));
addParameter(p, 'varsInc', [], @(x) iscell(x) || isempty(x));
addParameter(p, 'varsGrp', [], @(x) iscell(x) || isempty(x));
addParameter(p, 'flgZ', false, @islogical);
addParameter(p, 'varNorm', '', @ischar);
addParameter(p, 'flgLog', false, @islogical);
addParameter(p, 'logBase', 10, @(x) (isnumeric(x) && isscalar(x) && x > 0) || (ischar(x) && strcmp(x, 'e')));
addParameter(p, 'flgLogit', false, @islogical);
addParameter(p, 'flgNorm', false, @islogical);
addParameter(p, 'skewThr', 2, @(x) isnumeric(x) && isscalar(x));
addParameter(p, 'flg0', false, @islogical);
addParameter(p, 'verbose', false, @islogical);

parse(p, tbl, varargin{:});

varsExc = p.Results.varsExc;
varsInc = p.Results.varsInc;
varsGrp = p.Results.varsGrp;
varNorm = p.Results.varNorm;
flgZ = p.Results.flgZ;
flgLog = p.Results.flgLog;
logBase = p.Results.logBase;
flgLogit = p.Results.flgLogit;
flgNorm = p.Results.flgNorm;
skewThr = p.Results.skewThr;
flg0 = p.Results.flg0;
verbose = p.Results.verbose;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INITIALIZE VARIABLES & VALIDATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Get all variable names from the table
tblVars = tbl.Properties.VariableNames;
tblOut = tbl;

% Determine which variables to process
if ~isempty(varsInc)
    % Include only specified variables (takes precedence over varsExc)
    processVars = varsInc;
elseif ~isempty(varsExc)
    % Exclude specified variables
    excludeIdx = ismember(tblVars, varsExc);
    processVars = tblVars(~excludeIdx);
else
    % Process all variables
    processVars = tblVars;
end

% Filter to only numeric variables
numericIdx = cellfun(@(x) isnumeric(tbl.(x)), processVars);
processVars = processVars(numericIdx);

% Validation for Grouping and Normalization variables
if ~isempty(varsGrp)
    % Ensure varsGrp is cellstr
    if isstring(varsGrp)
        varsGrp = cellstr(varsGrp);
    end

    if ismember(varNorm, varsGrp)
        error('varNorm "%s" cannot be included in varsGrp', varNorm);
    end
end

% Warning for redundant transformations
if flgNorm && flgZ
    warning('Z-scoring will overwrite the scale set by normalization.');
end

% Get reference category (assuming first category is reference)
catRef = [];
if flgNorm
    if isempty(varNorm)
        error('flgNorm is true but varNorm is not specified.');
    end
    catRef = categories(tblOut.(varNorm));
    catRef = catRef{1};
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% GROUP INDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Check if group-based transformation is requested
if ~isempty(varsGrp)
    % Find unique combinations of grouping variables
    uGrps = unique(tblOut(:, varsGrp), 'rows');

    % Find all group indices in advance and store in cell array
    idxGrps = cell(height(uGrps), 1);
    for iGrp = 1:height(uGrps)
        uRow = uGrps(iGrp, :);

        % Find rows matching the current unique group combination
        idxGrp = true(height(tblOut), 1);
        for iVar = 1:length(varsGrp)
            idxGrp = idxGrp & (tblOut.(varsGrp{iVar}) == uRow.(varsGrp{iVar}));
        end
        idxGrps{iGrp} = idxGrp;
    end
else
    % Single group (all data)
    idxGrps = {true(height(tblOut), 1)};
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPLY TRANSFORMATIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

for iVar = 1:length(processVars)
    varName = processVars{iVar};
    varData = tblOut.(varName);

    % ---------------------------------------
    % Global Analysis (Logit, Log & Offset)
    % ---------------------------------------
    % Decisions for log transformation and offset must be made globally
    % to ensure the variable remains consistent across groups.

    if flgLogit
        varData = max(eps, min(1 - eps, varData));
        varData = log(varData ./ (1 - varData));
        if verbose
            fprintf('[%s] Applying Logit\n', varName);
        end
    end

    % Assert non-zero (for positive variables) if explicitly asked, or if
    % log transform is needed
    if flg0 || flgLog
        if any(varData == 0) && all(varData >= 0)
            c = min(varData(varData > 0)) / 2;
            varData = varData + c;
            if verbose
                fprintf('[%s] Adding Offset %.4f\n', varName, c);
            end
        end
    end

    if flgLog
        % Check skewness on the pooled data (ignoring NaNs)
        % Only consider positive skewness for log transform
        s = skewness(varData(~isnan(varData)));
        if s > skewThr

            % Determine base
            if ischar(logBase) && strcmp(logBase, 'e')
                varData = log(varData);
                baseStr = 'Ln';
            else
                varData = log(varData) ./ log(logBase);
                baseStr = sprintf('Log%.1f', logBase);
            end

            if verbose
                fprintf('[%s] Applying %s (Skew=%.2f)\n', varName, baseStr, s);
            end
        end
    end

    % Write back global changes before group-wise operations
    tblOut.(varName) = varData;

    % ---------------------------------------
    % Group-wise Operations (Norm & Z)
    % ---------------------------------------

    for iGrp = 1:length(idxGrps)
        idxGrp = idxGrps{iGrp};
        varData = tblOut.(varName)(idxGrp);

        % Normalize relative to reference category
        if flgNorm
            % Find rows within this group that match the reference category of varNorm
            idxRef = tblOut.(varNorm) == catRef;
            idxRefGroup = idxGrp & idxRef;

            % Calculate the mean for the reference category within this group
            refMean = mean(tblOut.(varName)(idxRefGroup), 'all', 'omitnan');

            % Check for issues with reference mean
            if refMean == 0
                refMean = eps; % Avoid division by zero, result will be large
            end
            if isnan(refMean)
                warning('[%s] Group %d RefMean is NaN. Skipping norm.\n', ...
                    varName, iGrp);
                continue
            end

            % Normalize
            varDiff = (varData - refMean) / abs(refMean);
            normData = 100 + varDiff * 100;

            % Update varData with normalized values
            varData = normData;
        end

        % Apply Z-scoring
        if flgZ
            varData = (varData - mean(varData, 'omitnan')) ./ std(varData, 'omitnan');
        end

        % Update the table column for this group
        tblOut.(varName)(idxGrp) = varData;
    end
end

end     % EOF


%% ========================================================================
%  NOTE: NORMALIZATION
%  ========================================================================
%  When `flgNorm` is true, this function standardizes values relative to a
%  specified reference group (e.g., 'Baseline' or 'Control'). The goal is
%  to express data as a percentage of the reference mean, where 100%
%  represents parity with the baseline.
%
%  The "Number Line" Approach:
%     Standard percent change formulas often fail when dealing with negative
%     values (e.g., calculating growth from -10 to -5). A simple ratio
%     can yield misleading signs. To resolve this, this function utilizes
%     the absolute value of the reference mean in the denominator:
%
%         Normalized = 100 + ((x - refMean) / abs(refMean)) * 100
%
%     This approach preserves the directionality of the change along the
%     real number line, regardless of the sign of the data:
%
%     1. Positive Domain: If Ref=10 and x=15, result is 150% (Increase).
%     2. Negative Domain: If Ref=-10 and x=-5, result is 150% (Increase).
%     3. Zero Crossing: If Ref=-5 and x=5, result is 300%.
%  ========================================================================


%% ========================================================================
%  NOTE: LOG VS. LOGIT
%  ========================================================================
%  Transforming the response variable is often necessary to satisfy the
%  assumption of normally distributed residuals in LME. The choice between
%  Log and Logit depends strictly on the domain boundaries of your data.
%
%  The Log Transform ( y -> log(y) ) is appropriate for magnitude data such
%  as Power, Energy, Duration, or Firing Rate, where the domain is positive
%  and theoretically unbounded [0, +Inf). These variables often exhibit a
%  multiplicative variance structure where variance grows with the mean,
%  along with a heavy right tail. The log function compresses this tail and
%  stabilizes the variance, mapping the data to (-Inf, +Inf). Note that
%  this transform is undefined for y <= 0, often requiring a shift if zeros
%  exist in the dataset.
%
%  The Logit Transform ( y -> log( y / (1-y) ) ) is required for data that
%  represents a bounded proportion or index, such as phase locking strength
%  (MRL) or probabilities, which are strictly bounded between [0, 1]. Such
%  data often exhibits variance compression near the boundaries, where
%  variance shrinks as y approaches 0 or 1. A simple Log transform only
%  fixes the lower bound but ignores the upper bound. The Logit transform
%  stretches both boundaries to infinity, effectively mapping the interval
%  [0, 1] to the whole real line (-Inf, +Inf). Implementing this requires
%  clipping exact 0s and 1s (e.g., to 0.001 and 0.999) to avoid infinite
%  values.
%
%  As a general decision rule, if your data can theoretically go to
%  infinity (e.g., Energy), use the Log transform. If your data represents
%  a strength of locking or index capped at 1 (e.g., Mean Resultant
%  Length), use the Logit transform.
%  ========================================================================


%% ========================================================================
%  NOTE: MODULATION INDEX (MI) VS. LOG-RATIO
%  ========================================================================
%  When quantifying firing rate changes (e.g., Ripple vs. Random periods), 
%  the choice of metric significantly impacts the statistical validity 
%  of the subsequent Linear Mixed-Effects (LME) analysis.
%
%  1. The Modulation Index (MI):
%     Defined as (FR_a - FR_b) / (FR_a + FR_b). This metric is a standard
%     "normalized difference" in neuroscience because it is intuitive and
%     controls for the absolute firing rate of a neuron. However, it
%     presents challenges for LME models. Because it is strictly bounded
%     between [-1, 1], it violates the assumption of normality, especially
%     near the boundaries. Analyzing MI typically requires rescaling the
%     data to (0, 1) and employing a Logit-Normal model.
%
%  2. The Log-Ratio (LR):
%     Defined as ln(FR_a / FR_b). This is often the statistically preferred
%     alternative to the MI. Unlike the MI, the Log-Ratio is not bounded
%     and is naturally symmetric around zero, which often results in
%     residuals that better approximate a Normal distribution. This
%     symmetry aligns with the "Log Link" philosophy, as it models
%     multiplicative processes on a linear scale.
%
%  3. The GLMM Approach (Raw Rates):
%     The most rigorous method is to avoid index-based metrics entirely. By
%     fitting a Generalized Linear Mixed Model (GLMM) directly to the raw
%     firing rates (e.g., using Gamma or Inverse Gaussian distributions),
%     one preserves the inherent mean-variance relationship of the data
%    . This approach accounts for the "multiplicative" nature 
%     of neural spiking while avoiding the information loss and potential 
%     biases (like the "retransformation problem") associated with 
%     collapsing observations into a single ratio.
%
%  DECISION RULE: Use the Modulation Index (MI) primarily for visualization 
%  and consistency with literature. For LME modeling, favor the **Log-Ratio** %  for simplicity of inference, or a **GLMM on raw rates** (Gamma/InvGauss) 
%  to ensure the most accurate and unbiased population estimates.
%  ========================================================================